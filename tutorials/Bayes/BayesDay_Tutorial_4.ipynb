{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT6vt0dDy0Ad"
   },
   "source": [
    "## Neuromatch Academy 2020 -- Bayes Day (dry run)\n",
    "# Tutorial 4 - Marginalization & Fitting to data\n",
    "\n",
    "Please execute the cell below to initialize the notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "P5hOGNx3y0Ae"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import time                        # import time \n",
    "import numpy as np                 # import numpy\n",
    "import scipy as sp                 # import scipy\n",
    "import math                        # import basic math functions\n",
    "import random                      # import basic random number generator functions\n",
    "\n",
    "import matplotlib.pyplot as plt    # import matplotlib\n",
    "import matplotlib as mpl\n",
    "from IPython import display        \n",
    "from scipy.optimize import minimize    \n",
    "\n",
    "fig_w, fig_h = (6, 4)\n",
    "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
    "#plt.style.use('ggplot')\n",
    "mpl.rc('figure', max_open_warning = 0)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "def my_gaussian(x_points, mu, sigma):\n",
    "    \"\"\"\n",
    "    Returns un-normalized Gaussian estimated at points `x_points`, with parameters: `mu` and `sigma`\n",
    "    \n",
    "    Args :\n",
    "      x_points (numpy arrays of floats)- points at which the gaussian is evaluated\n",
    "      mu (scalar) - mean of the Gaussian\n",
    "      sigma (scalar) - std of the gaussian\n",
    "\n",
    "    Returns: \n",
    "      un-normalized Gaussian (i.e. without constant) evaluated at `x`\n",
    "    \"\"\"\n",
    "    return np.exp(-(x_points-mu)**2/(2*sigma**2))\n",
    "\n",
    "def moments_myfunc(x_points, function):\n",
    "  \"\"\"\n",
    "  DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "  Returns the mean, median and mode of an arbitrary function\n",
    "\n",
    "  Args : \n",
    "    x_points (numpy array of floats) - x-axis values\n",
    "    function (numpy array of floats) - y-axis values of the function evaluated at `x_points`\n",
    "\n",
    "  Returns:\n",
    "    (tuple of 3 scalars): mean, median, mode\n",
    "  \"\"\"\n",
    "  \n",
    "  # Calc mode of arbitrary function\n",
    "  mode = x_points[np.argmax(function)]\n",
    "\n",
    "  # Calc mean of arbitrary function\n",
    "  mean = np.sum(x_points * function)\n",
    "\n",
    "  # Calc median of arbitrary function\n",
    "  cdf_function = np.zeros_like(x_points)\n",
    "  accumulator = 0\n",
    "  for i in np.arange(x.shape[0]):\n",
    "    accumulator = accumulator + function[i]\n",
    "    cdf_function[i] = accumulator\n",
    "  idx = np.argmin(np.abs(cdf_function - 0.5))\n",
    "  median = x_points[idx]\n",
    "\n",
    "  return mean, median, mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "92DKxQWIy0Ai"
   },
   "source": [
    "---\n",
    "\n",
    "### Tutorial objectives\n",
    "  \n",
    "In this notebook we'll have a look at computing the Marginalization Matrix and the Marginal in order to perform model inversion (i.e.: recovering the model parameters given a participant's data). \n",
    "\n",
    "The generative model will be the same Bayesian model we have been using in Tutorial 3 (Mixture of Gaussian Prior and Gaussian Likelihood).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bd_SCerk3fqC"
   },
   "source": [
    "---\n",
    "### EXERCISE 1: Mixture of Gaussian prior\n",
    "   \n",
    "Similarly to Tutorial 2, we now want to create a prior matrix using a mixture of gaussians prior.\n",
    "\n",
    "**Suggestions**\n",
    "\n",
    "Using the equation for the un-normalised Gaussian `my_gaussian`\n",
    "* Generate a Gaussian with mean 0 and standard deviation 0.5\n",
    "* Generate another Gaussian with mean 0 and standard deviation 10\n",
    "* Combine the two Gaussians to make a new prior by mixing the two Gaussians with mixing parameter alpha = 0.05. Make it such that the peakier Gaussian has 95% of the weight (don't forget to normalize afterwards)\n",
    "* This will be the first row of your prior matrix\n",
    "* Now repeat (hint: use np.tile) that row prior to make a matrix of 1000 (i.e. `hypothetical_stim.shape[0]`)  row-priors.\n",
    "* Plot the matrix using the function `plt.matshow` already pre-written and commented-out in your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHVfOLCv5Ibc"
   },
   "outputs": [],
   "source": [
    "hypothetical_stim = np.linspace(-8,8,1000)\n",
    "x = np.arange(-10,10,0.1)\n",
    "\n",
    "##################\n",
    "## Insert your code here to:\n",
    "##       - Generate a mixture of gaussian prior with mean 0 and std 0.5 and 10 respectively\n",
    "##       - Tile that row prior in order to make a matrix of 1000 row priors\n",
    "##         (Hint: use np.tile() and np.reshape())\n",
    "##       - Plot the Prior Matrix using the code snippet commented-out below\n",
    "##################\n",
    "\n",
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(prior_matrix)\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_title('Prior Matrix: p(x)')\n",
    "# ax.set_ylabel('Repetitions')\n",
    "# ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xuhqR9Oz6uPh"
   },
   "source": [
    "---\n",
    "### EXERCISE 2: Implement a Likelihood Matrix\n",
    "    \n",
    "We now want to create a Likelihood matrix that is made up of a Gaussian on each row of the matrix. Each row represents a different hypothetically presented stimulus with a different stimulus offset (i.e. a different likelihood mean).\n",
    "\n",
    "**Suggestions**\n",
    "\n",
    "  Using the equation for the un-normalised Gaussian `my_gaussian` and the values in `hypothetical_stim`:\n",
    "* Create a Gaussian likelihood with mean varying from `hypothetical_stim`, keeping $\\sigma$ constant at 1.\n",
    "* Each Likelihood with a different mean will make up a different row-likelihood of your matrix, such that you end up with a Likelihood matrix made up of 1000 row-Gaussians with different means.\n",
    "* Plot the matrix using the function `plt.matshow` already pre-written and commented-out in your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLLJ0UIr7dae"
   },
   "outputs": [],
   "source": [
    "likelihood_matrix = np.zeros_like(prior_matrix)\n",
    "\n",
    "##################\n",
    "## Insert your code here to:\n",
    "##       - Generate a likelihood matrix using `my_gaussian` function, with sigma = 1,\n",
    "##         and varying the mean using `hypothetical_stim` values.\n",
    "##       - Plot the Prior Matrix using the code snippet commented-out below\n",
    "##################\n",
    "\n",
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(likelihood_matrix)\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_title('Likelihood Matrix : p(x_tilde|x)')\n",
    "# ax.set_ylabel('x_tilde : Brain representation of x')\n",
    "# ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uUC3iZH8i5B"
   },
   "source": [
    "---\n",
    "### EXERCISE 3: Implement the Posterior Matrix\n",
    "    \n",
    "We now want to create the Posterior matrix. To do so, we will compute the posterior using *Bayes rule* for each trial (i.e. row_wise).\n",
    "\n",
    "That is, each row of the posterior matrix will be the posterior resulting from the multiplication of the prior and likelihood of the equivalent row.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    Posterior\\left[i, :\\right] \\propto Likelihood\\left[i, :\\right] \\odot Prior\\left[i, :\\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\odot$ represent the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (i.e. the element_wise multiplication) of the Prior and Likelihood row vectors `i` from the matrix.\n",
    "\n",
    "**Suggestions**\n",
    "\n",
    "* For each row (trial) of the Prior and Likelihood matrix, calculate posterior and fill in the Posterior matrix, such that each row of the Posterior matrix is represent the posterior for a different trial.\n",
    "* Plot the matrix using the function `plt.matshow` already pre-written and commented-out in your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-3Dgk8E8jSc"
   },
   "outputs": [],
   "source": [
    "posterior_matrix = np.zeros_like(likelihood_matrix)\n",
    "\n",
    "###############################################################################\n",
    "## Insert your code here to:\n",
    "##        For each row of the Prior & Likelihood Matrices, calculate the resulting posterior \n",
    "##        Fill the Posterior Matrix with the row_posterior\n",
    "##        Plot the Posterior Matrix using the code snippet provided below\n",
    "###############################################################################\n",
    "\n",
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(posterior_matrix)\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_title('Posterior Matrix : p(x | x_tilde)')\n",
    "# ax.set_ylabel('x_tilde')\n",
    "# ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8T_yJRH96vb"
   },
   "source": [
    "---\n",
    "### EXERCISE 4: Implement the Binary Decision Matrix\n",
    "    \n",
    "We now want to create the a Binary Decision Matrix. To do so, we will scan the Posterior matrix (i.e. row_wise), and set the matrix cell to 1 at the mean of the row posterior.\n",
    "\n",
    "\n",
    "This, effectively encodes the *decision* that a participant may make on a given trial (i.e. row). In this case, the modelled decision rule is to take the mean of the posterior on each trial (use the function `moments_myfunc()` provided to calculate the mean of the posterior).\n",
    "\n",
    "**Suggestions**\n",
    "* For each row (trial) of the Posterior matrix, calculate the mode of the posterior, and set the corresponding cell of the Binary Decision Matrix to 1. (e.g. if the mode of the posterior is at position 0, then set the cell with x_column == 0 to 1).\n",
    "* Plot the matrix using the function `plt.matshow` already pre-written and commented-out in your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3WfAggw9679"
   },
   "outputs": [],
   "source": [
    "binary_decision_matrix = np.zeros_like(posterior_matrix)\n",
    "\n",
    "###############################################################################\n",
    "## Insert your code here to:\n",
    "##        Create a matrix of the same size as the Posterior matrix and fill it with zeros (Hint: use np.zeros_like())\n",
    "##        For each row of the Posterior Matrix, calculate the mean of the posterior using the function povided `moments_myfunc()`, and set the corresponding cell of the Binary Decision Matrix to 1. \n",
    "##        Plot the Posterior Matrix using the function `plt.pcolor` and the code snippet provided below\n",
    "###############################################################################\n",
    "\n",
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(binary_decision_matrix)\n",
    "# ax.set_xlabel('x_tilde')\n",
    "# ax.set_title('Binary Decision Matrix : x_hat = mean(x_tilde)')\n",
    "# ax.set_ylabel('x_hat')\n",
    "# ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCFp0Hzq-5ot"
   },
   "source": [
    "---\n",
    "### EXERCISE 5: Implement the Input Matrix\n",
    "    \n",
    "We now want to create the Input Matrix from the true presented stimulus. That is, we will now create a Gaussian centered around the true presented stimulus, with sigma = 1. and repeat that gaussian distribution across x values. That is we want to make a *Column* gaussian centered around the true presented stimulus, and repeat this *Column* Gaussian across all values of the x-axis matrix.\n",
    "\n",
    "This, effectively encodes the distribution of the true stimulus (one single simulus) for that a participant on a given trial. \n",
    "\n",
    "**Suggestions**\n",
    "\n",
    "Assume the true stimulus is presented at direction -2.5\n",
    "* Create a Gaussian likelihood with mean = -2.5 with $\\sigma$ constant at 1.\n",
    "* Make this the first column of your Matrix and repeat that *column* Gaussian to fill in the True_Presented_Stimulus Matrix.\n",
    "* Plot the matrix using the function `plt.matshow` already pre-written and commented-out in your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iv5nF8ln-5-j"
   },
   "outputs": [],
   "source": [
    "input_matrix = np.zeros_like(posterior_matrix)\n",
    "\n",
    "##################\n",
    "## Insert your code here to:\n",
    "##       - Generate a gaussian centered on the true stimulus -2.5 with sigma = 1\n",
    "##       - Tile that column input Gaussian in order to complete the matrix\n",
    "##         (Hint: use np.tile() and np.reshape())\n",
    "##       - Plot the Matrix using the code snippet commented-out below\n",
    "##################\n",
    "\n",
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(input_matrix)\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_title('Input Matrix: p(x_tilde | x = -2.5)')\n",
    "# ax.set_ylabel('x_tilde')\n",
    "# ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "va03CJiXBfjD"
   },
   "source": [
    "---\n",
    "### EXERCISE 5: Implement the Marginalization Matrix\n",
    "    \n",
    "We now want to compute the Marginalization Matrix from the true presented stimulus, and our Binary decision matrix over hypothetical stimulus inputs. \n",
    "\n",
    "Mathematically, this means that we want to compute:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    Marginalization Matrix = Input Matrix \\odot Binary Matrix\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    Marginal = \\int_{x} Marginalization Matrix\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\odot$ represent the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (i.e. the element_wise multiplication) of the Input matrix and Binary matrix.\n",
    "\n",
    "**Suggestions**\n",
    "\n",
    "* For each row of the Input and Binary matrix, calculate product of the two and fill in the Marginal matrix.\n",
    "* Plot the matrix using the function `plt.matshow` already pre-written and commented-out in your script\n",
    "* Calcualte and plot the Marginal over `x` using the code snippet commented out in your script\n",
    "   - Note how the limitations of numerical integration create artifacts on your marginal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm13p_5SBfwv"
   },
   "outputs": [],
   "source": [
    "marginalization_matrix = np.zeros_like(posterior_matrix)\n",
    "\n",
    "###############################################################################\n",
    "## Insert your code here to:\n",
    "##        Compute the Marginalization matrix by multiplying pointwise the Binary decision matrix over hypothetical stimuli and the Input Matrix\n",
    "##        Compute the Marginal from the Marginalization matrix by summing over x (hint: use np.sum())\n",
    "##        Plot the Marginalization Matrix and the resulting Marginal using the code snippet provided below\n",
    "###############################################################################\n",
    "\n",
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(marginalization_matrix)\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_title('Marginalization Matrix: p(x_hat | x)')\n",
    "# ax.set_ylabel('x_hat')\n",
    "# ax.set_aspect('auto')\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.plot(x, marginal)\n",
    "# plt.xlabel('x_hat')\n",
    "# plt.ylabel('probability')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ch2KJ5O6zxW5"
   },
   "source": [
    "---\n",
    "### EXERCISE 6: Generate some Data\n",
    "\n",
    "Now that we've seen how to calculate the posterior and marginalize to get $p(\\hat{x} \\mid x)$ we will generate some artificial data for a single participant using the `generate_data()` function provided, and mixing parameter $\\alpha$ = 0.1\n",
    "\n",
    "Please run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "G_jf5EOOzwyo",
    "outputId": "b7936f30-ea77-41a6-8764-6e7842a1dde8"
   },
   "outputs": [],
   "source": [
    "def generate_data(x_stim, alpha):\n",
    "  \"\"\"\n",
    "  DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "  Returns the mean, median and mode of an arbitrary function\n",
    "\n",
    "  Args : \n",
    "    x_stim (numpy array of floats) - x values at which stimuli are presented\n",
    "    alpha (scalar) - mixture component for the Mixture of Gaussian prior\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats): x_hat response of participant for each stimulus\n",
    "  \"\"\"\n",
    "  x = np.arange(-10,10,0.1)\n",
    "  x_hat = np.zeros_like(x_stim)\n",
    "\n",
    "  prior_mean = 0\n",
    "  prior_sigma1  = .5\n",
    "  prior_sigma2  = 3\n",
    "  prior1 = my_gaussian(x, prior_mean, prior_sigma1)\n",
    "  prior2 = my_gaussian(x, prior_mean, prior_sigma2)\n",
    "\n",
    "  prior_combined = (1-alpha) * prior1 + (alpha * prior2) \n",
    "  prior_combined = prior_combined / np.sum(prior_combined)\n",
    "\n",
    "  for i_stim in np.arange(x_stim.shape[0]):\n",
    "    likelihood_mean = x_stim[i_stim]\n",
    "    likelihood_sigma  = 1\n",
    "    likelihood = my_gaussian(x, likelihood_mean, likelihood_sigma)\n",
    "    likelihood = likelihood / np.sum(likelihood)\n",
    "\n",
    "    posterior = np.multiply(prior_combined, likelihood)\n",
    "    posterior = posterior / np.sum(posterior)\n",
    "    \n",
    "    # Assumes participant takes posterior mean as 'action'\n",
    "    x_hat[i_stim] = np.sum(x * posterior)\n",
    "  return x_hat\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Generate data for a single participant\n",
    "true_stim = np.array([-8, -4, -3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 4, 8])\n",
    "behaviour = generate_data(true_stim, 0.1)\n",
    "\n",
    "# Plot of data\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(true_stim, true_stim - behaviour, '-k', linewidth=2, label='data')\n",
    "plt.legend()\n",
    "plt.xlabel('Position of true visual stimulus (cm)')\n",
    "plt.ylabel('Participant deviation from true stimulus (cm)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIbUCb_4znvR"
   },
   "source": [
    "---\n",
    "### EXERCISE 7: Model fitting to generated data\n",
    "\n",
    "Now that we have generated some data and that we have seen how to calculate the posterior and marginalize to get $p(\\hat{x} \\mid x)$ we will attempt to recover the parameter alpha = 0.05 that was use to generate the data.\n",
    "\n",
    "We have provided you with an incomplete function called `my_Bayes_model_mse()` that needs to be completed to perform the same computations you have performed in the previous exercises but over all the participant's trial, as opposed to a single trial.\n",
    "\n",
    "The Likelihood, has already been constructed and will not change since they depend on hypothetical stimuli. We will however have to implement the Prior matrix since it depends on $\\alpha$ as well as recomputing the Posterior, Input and the marginalization matrix to compute the marginal in order to get $p(\\hat{x} \\mid x)$. Using $p(\\hat{x} \\mid x)$, we can then compute the negative Log-Likelihood for each trial in order to find the parameter that minimizes the negative Log-Likelihood (i.e. Maximises the Log-Likelihood).\n",
    "\n",
    "Trials are assumed to be independent from one another. Mathematically this means that we can define the negative Log-Likelihood as:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    -LL = - \\sum_i \\log p(\\hat{x_i} \\mid x_i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\hat{x_i}$ is the participant's response for trial $i$, with presented stimulus $x_i$ \n",
    "\n",
    "**Suggestions**\n",
    "\n",
    "* Complete the function my_Bayes_model_mse, to calculate the Prior, Posterior,Input and Marginalization matrix on each trial\n",
    "* Compute the marginal using the marginalization matrix on each trial\n",
    "* Compute the negative log likelihood using the marginal and the participant's response\n",
    "* Using the code snippet commented out in your script to loop over possible values of $\\alpha$\n",
    "* Bonus question : Use the optimization function `minimize` to find the optimal value of $\\alpha$.\n",
    "   - Note how the limitations of numerical integration create artifacts on your marginal and the resulting negative Log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "PaJf0puXLb7H",
    "outputId": "810ae3c8-a591-4e79-f594-79ae8a7e7a41"
   },
   "outputs": [],
   "source": [
    "def my_Bayes_model_mse(params):\n",
    "  \"\"\"\n",
    "  Function fits the Bayesian model from Tutorial 3 \n",
    "  \n",
    "  Args : \n",
    "    params (list of positive floats):  parameters used by the model (params[0]  = posterior scaling)\n",
    "            \n",
    "  Returns :\n",
    "    (scalar) negative log-likelihood :sum of log probabilities\n",
    "  \"\"\"\n",
    "  \n",
    "\n",
    "  trial_ll = np.zeros_like(true_stim)\n",
    "\n",
    "  ###############################################################################\n",
    "  ## Insert your code here to:\n",
    "  ##        Compute the Prior matrix given `alpha`\n",
    "  ##        Compute the Posterior matrix\n",
    "  ##        Compute the Binary decision matrix\n",
    "  ###############################################################################\n",
    "\n",
    "  # Loop over stimuli\n",
    "  for i_stim in np.arange(true_stim.shape[0]):\n",
    "    \n",
    "    ###############################################################################\n",
    "    ## Insert your code here to:\n",
    "    ##        Compute the Input matrix \n",
    "    ##        Compute the Marginalization matrix\n",
    "    ##        Compute the Marginal\n",
    "    ##        Compute and return the negative log likelihood of the participant\n",
    "    ###############################################################################\n",
    "    raise NotImplementedError(\"You need to complete this function!\")\n",
    "\n",
    "x = np.arange(-10,10,0.1)\n",
    "\n",
    "# Plot neg-LogLikelihood for different values of alpha\n",
    "alpha_tries = np.arange(0.01,0.3,0.01)\n",
    "nll = np.zeros_like(alpha_tries)\n",
    "for i_try in np.arange(alpha_tries.shape[0]):\n",
    "  nll[i_try] = my_Bayes_model_mse(np.array([alpha_tries[i_try]]))\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(alpha_tries, nll)\n",
    "plt.xlabel('alpha value')\n",
    "plt.ylabel('negative log-likelihood')\n",
    "plt.axvline(alpha_tries[np.argmin(nll)])\n",
    "plt.show()\n",
    "print(f\"Best parameters estimated, scaling parameter alpha: {alpha_tries[np.argmin(nll)]:.2f}\")\n",
    "\n",
    "# Parameters for optimization\n",
    "x0 = [0.05]         # Initial guess for parameters\n",
    "bounds = [(0.01,1)] # Optimization bounds\n",
    "\n",
    "# result = minimize(...)\n",
    "# print(f\"Best parameters estimated using minimization function, scaling parameter alpha: {result.x[0]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Tutorial4_BAYESDAY_colab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
